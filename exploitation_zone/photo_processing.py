# photo_processing.py

import os
import json
import io
from datetime import datetime

from minio import Minio
from pyspark.sql import SparkSession
from pyspark.sql.types import (
    StructType, StructField, StringType,
    ArrayType, FloatType, TimestampType
)
from pinecone import Pinecone, ServerlessSpec


BUCKET              = os.environ.get("BUCKET", "trusted-photos")
PREFIX              = os.environ.get("PREFIX", "validated/embeddings/")
DELTA_OUTPUT_PATH   = os.environ.get("DELTA_OUTPUT_PATH", "/data/delta/photo_embeddings")
PINECONE_API_KEY    = os.environ["PINECONE_API_KEY"]
PINECONE_ENV        = os.environ.get("PINECONE_ENV", "us-east-1")
PINECONE_INDEX_NAME = os.environ.get("PINECONE_INDEX_NAME", "photo-embeddings")
EMBEDDING_DIM       = int(os.environ.get("EMBEDDING_DIM", "512"))

# Initialize MinIO client
minio_client = Minio(
    "minio:9000",
    access_key="minioadmin",
    secret_key="minioadmin",
    secure=False
)

# Initialize Spark
spark = (
    SparkSession.builder
    .appName("Process and Upload Photo Embeddings")
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
    .getOrCreate()
)

# ——— NEW PINECONE SETUP ———

pc = Pinecone(
    api_key=PINECONE_API_KEY,
    environment=PINECONE_ENV
)

# Create the index if it doesn't exist
existing = pc.list_indexes().names()
if PINECONE_INDEX_NAME not in existing:
    pc.create_index(
    name=PINECONE_INDEX_NAME,
    dimension=EMBEDDING_DIM,
    metric="cosine",
    spec=ServerlessSpec(
        cloud="aws",        
        region=PINECONE_ENV   
    )
)
# Get a handle to that index
index = pc.Index(PINECONE_INDEX_NAME)


def parse_embedding_data(embedding_data: str):
    try:
        parsed = json.loads(embedding_data)
        if isinstance(parsed, list) and len(parsed) == EMBEDDING_DIM:
            return [float(x) for x in parsed]
    except Exception as e:
        print(f"Failed to parse embedding JSON: {e}")
    return None


def process_embeddings_to_delta_and_pinecone():
    rows = []
    pinecone_vectors = []

    for obj in minio_client.list_objects(BUCKET, prefix=PREFIX, recursive=True):
        if obj.object_name.lower().endswith(".json"):
            try:
                resp = minio_client.get_object(BUCKET, obj.object_name)
                data = resp.read().decode("utf-8")
                emb = parse_embedding_data(data)
                resp.close()

                if emb:
                    photo_id = os.path.splitext(os.path.basename(obj.object_name))[0]
                    student_id = os.path.basename(os.path.dirname(obj.object_name))

                    # collect for Delta write
                    rows.append((
                      photo_id,
                      student_id,
                      emb,
                      datetime.utcnow()
                    ))

                    # collect for Pinecone upsert
                    pinecone_vectors.append((photo_id, emb))
                else:
                    print(f"Skipping invalid embedding: {obj.object_name}")

            except Exception as e:
                print(f"Error reading {obj.object_name}: {e}")

    # Write to Delta Lake
    if rows:
        schema = StructType([
            StructField("photo_id",      StringType()),
            StructField("student_id",    StringType()),
            StructField("embedding",     ArrayType(FloatType())),
            StructField("processed_time", TimestampType())
        ])
        df = spark.createDataFrame(rows, schema)
        df.write.format("delta").mode("overwrite").save(DELTA_OUTPUT_PATH)
        print(f"Wrote {len(rows)} embeddings to Delta at {DELTA_OUTPUT_PATH}")

    # Upsert into Pinecone
    if pinecone_vectors:
        index.upsert(vectors=pinecone_vectors)
        print(f"Upserted {len(pinecone_vectors)} vectors to Pinecone '{PINECONE_INDEX_NAME}'")


if __name__ == "__main__":
    process_embeddings_to_delta_and_pinecone()
    spark.stop()
