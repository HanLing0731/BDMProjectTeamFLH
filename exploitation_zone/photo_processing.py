import os
import json
from datetime import datetime

from minio import Minio
from pyspark.sql import SparkSession
from pyspark.sql.types import (
    StructType,
    StructField,
    StringType,
    ArrayType,
    FloatType,
    TimestampType,
)
from pinecone import Pinecone, ServerlessSpec

# ── Configuration via env vars ────────────────────────────────────────────────
BUCKET            = os.environ.get("BUCKET", "trusted-photos")
PREFIX            = os.environ.get("PREFIX", "validated/embeddings/")
DELTA_OUTPUT_PATH = os.environ.get("DELTA_OUTPUT_PATH", "/data/delta/photo_embeddings")

API_KEY           = os.environ["PINECONE_API_KEY"]
ENV               = os.environ["PINECONE_ENV"]
INDEX_NAME        = os.environ.get("PINECONE_INDEX_NAME", "photo-embeddings")
DIM               = int(os.environ.get("EMBEDDING_DIM", "512"))

# ── MinIO client ──────────────────────────────────────────────────────────────
minio_client = Minio(
    "minio:9000",
    access_key="minioadmin",
    secret_key="minioadmin",
    secure=False
)

# ── Spark session (with Delta Lake support) ───────────────────────────────────
spark = (
    SparkSession.builder
        .appName("Process and Upload Photo Embeddings")
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
        .getOrCreate()
)

# ── Pinecone client ────────────────────────────────────────────────────────────
pc = Pinecone(api_key=API_KEY, environment=ENV)

# create the index if it doesn't exist
existing = pc.list_indexes().names()
if INDEX_NAME not in existing:
    pc.create_index(
        name=INDEX_NAME,
        dimension=DIM,
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region=ENV)
    )

index = pc.Index(INDEX_NAME)

# ── Helpers ────────────────────────────────────────────────────────────────────
def parse_embedding_data(embedding_data: str):
    """
    Expect a JSON array of length DIM; return as list[float] or None if invalid.
    """
    try:
        parsed = json.loads(embedding_data)
        if isinstance(parsed, list) and len(parsed) == DIM:
            return [float(x) for x in parsed]
    except Exception as e:
        print(f"Failed to parse embedding JSON: {e}")
    return None

# ── Main processing ───────────────────────────────────────────────────────────
def process_embeddings_to_delta_and_pinecone():
    rows = []
    pinecone_vectors = []

    for obj in minio_client.list_objects(BUCKET, prefix=PREFIX, recursive=True):
        if not obj.object_name.lower().endswith(".json"):
            continue
        try:
            resp = minio_client.get_object(BUCKET, obj.object_name)
            raw = resp.read().decode("utf-8")
            resp.close()

            emb = parse_embedding_data(raw)
            if emb is None:
                print(f"Skipping invalid embedding file: {obj.object_name}")
                continue

            # Derive IDs from path: trusted-photos/{student_id}/{photo_id}.json
            photo_id = os.path.splitext(os.path.basename(obj.object_name))[0]
            student_id = os.path.basename(os.path.dirname(obj.object_name))

            # Collect for Delta write
            rows.append((photo_id, student_id, emb, datetime.utcnow()))
            # Collect for Pinecone upsert
            pinecone_vectors.append((photo_id, emb))

        except Exception as e:
            print(f"Error reading {obj.object_name}: {e}")

    # Write to Delta Lake
    if rows:
        schema = StructType([
            StructField("photo_id",      StringType(),    nullable=False),
            StructField("student_id",    StringType(),    nullable=False),
            StructField("embedding",     ArrayType(FloatType()), nullable=False),
            StructField("processed_time", TimestampType(), nullable=False),
        ])
        df = spark.createDataFrame(rows, schema)
        df.write.format("delta").mode("overwrite").save(DELTA_OUTPUT_PATH)
        print(f"Wrote {len(rows)} embeddings to Delta at {DELTA_OUTPUT_PATH}")

    # Upsert into Pinecone
    if pinecone_vectors:
        index.upsert(vectors=pinecone_vectors)
        print(f"Upserted {len(pinecone_vectors)} vectors into Pinecone index '{INDEX_NAME}'")

# ── Entry point ────────────────────────────────────────────────────────────────
if __name__ == "__main__":
    process_embeddings_to_delta_and_pinecone()
    spark.stop()
